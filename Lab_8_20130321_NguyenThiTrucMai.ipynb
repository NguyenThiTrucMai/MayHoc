{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NguyenThiTrucMai/MayHoc/blob/main/Lab_8_20130321_NguyenThiTrucMai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMzehe0sy5wr"
      },
      "source": [
        "# This lab deals with **GridSearchCV** for tuning the hyper-parameters of an estimator and applying vectorization techniques to the **movie reviews dataset** for classification task. \n",
        "\n",
        "*   **Deadline: 23:59, 17/4/2023**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4nJmxp9zGX4"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "DoVWQ8AEyc-C"
      },
      "outputs": [],
      "source": [
        "# code\n",
        "\n",
        "from sklearn import datasets\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics._plot.confusion_matrix import ConfusionMatrixDisplay\n",
        "from sklearn import metrics\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import tree\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn import svm\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.feature_selection import f_classif\n",
        "\n",
        "\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# importing libraries\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import datasets\n",
        "\n",
        "\n",
        "# code\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import datasets\n",
        "\n",
        "from scipy.stats import mode\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# code\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import datasets\n",
        "from scipy.stats import mode\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "import scipy.cluster.hierarchy as shc\n",
        "\n",
        "from sklearn.metrics.cluster import silhouette_score\n",
        "from plotnine import ggplot, aes, geom_line,geom_point\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import scipy.cluster.hierarchy as sch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoCuBag162XR"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_dG9SA5OhGT"
      },
      "source": [
        "#Task 1. With **iris** dataset\n",
        "*  1.1. Apply **GridSearchCV** for **SVM** to find the best hyperparameters using the following param_grid.\n",
        "\n",
        "```\n",
        "param_grid = {'C': [0.1, 1, 10, 100, 1000],\n",
        "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
        "              'kernel': ['rbf','linear']}\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62jExOZ952fF"
      },
      "outputs": [],
      "source": [
        "#code\n",
        "\n",
        "dataset = datasets.load_iris()\n",
        "X= dataset['data']\n",
        "y= dataset['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VXmr_on6Ny5"
      },
      "outputs": [],
      "source": [
        "param_grid = {'C': [0.1, 1, 10, 100, 1000],\n",
        "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
        "              'kernel': ['rbf','linear']}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YPgC3rXDB_Vv",
        "outputId": "ea14fb73-e3c2-438b-e30c-58774a9c0383"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        16\n",
            "           1       1.00      0.94      0.97        18\n",
            "           2       0.92      1.00      0.96        11\n",
            "\n",
            "    accuracy                           0.98        45\n",
            "   macro avg       0.97      0.98      0.98        45\n",
            "weighted avg       0.98      0.98      0.98        45\n",
            "\n"
          ]
        }
      ],
      "source": [
        "clf = svm.SVC(kernel='linear',random_state=0, max_iter=10000)\n",
        "grid_svm_class=GridSearchCV(estimator=clf,param_grid=param_grid,scoring='accuracy',refit=True,return_train_score=True)\n",
        "\n",
        "grid_svm_class.fit(X_train,y_train)\n",
        "y_predict= grid_svm_class.predict(X_test)\n",
        "print(metrics.classification_report(y_test,y_predict))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g--8cng53sY"
      },
      "source": [
        "*  1.2. Apply **GridSearchCV** for **kNN** to find the best hyperparameters using the following param_grid.\n",
        "\n",
        "```\n",
        "grid_params = { 'n_neighbors' : [5,7,9,11,13,15],\n",
        "               'weights' : ['uniform','distance'],\n",
        "               'metric' : ['minkowski','euclidean','manhattan']}\n",
        "```\n",
        "where\n",
        "\n",
        "    *  **n_neighbors**: Decide the best k based on the values we have computed earlier.\n",
        "    *  **weights**: Check whether adding weights to the data points is beneficial to the model or not. 'uniform' assigns no weight, while 'distance' weighs points by the inverse of their distances meaning nearer points will have more weight than the farther points.\n",
        "    *  **metric**: The distance metric to be used will calculating the similarity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fX0_kItYPism"
      },
      "outputs": [],
      "source": [
        "#code\n",
        "\n",
        "grid_params = { 'n_neighbors' : [5,7,9,11,13,15],\n",
        "               'weights' : ['uniform','distance'],\n",
        "               'metric' : ['minkowski','euclidean','manhattan']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ddTU6T2yCGgO",
        "outputId": "727eaf01-e899-4c25-a044-40ea1b98b737"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        16\n",
            "           1       1.00      0.94      0.97        18\n",
            "           2       0.92      1.00      0.96        11\n",
            "\n",
            "    accuracy                           0.98        45\n",
            "   macro avg       0.97      0.98      0.98        45\n",
            "weighted avg       0.98      0.98      0.98        45\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred_knn = knn.predict(X_test)\n",
        "\n",
        "# accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
        "# precision_knn = precision_score(y_test, y_pred_knn, average='macro')\n",
        "# recall_knn = recall_score(y_test, y_pred_knn, average='macro')\n",
        "# f1_knn = f1_score(y_test, y_pred_knn, average='macro')\n",
        "\n",
        "grid_KNN_class=GridSearchCV(estimator=knn,param_grid=grid_params,scoring='accuracy',refit=True,return_train_score=True)\n",
        "\n",
        "grid_KNN_class.fit(X_train,y_train)\n",
        "y_predict= grid_KNN_class.predict(X_test)\n",
        "print(metrics.classification_report(y_test,y_predict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lQSOcjL_TIW"
      },
      "source": [
        "*  1.3. Apply **GridSearchCV** for **Random Forest** to find the best hyperparameters using the following param_grid.\n",
        "\n",
        "```\n",
        "param_grid = {\n",
        "    'n_estimators': [25, 50, 100, 150],\n",
        "    'max_features': ['sqrt', 'log2', None],\n",
        "    'max_depth': [3, 6, 9],\n",
        "    'max_leaf_nodes': [3, 6, 9],\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "OlyF9WpN_01p"
      },
      "outputs": [],
      "source": [
        "#code\n",
        "param_grid = {\n",
        "    'n_estimators': [25, 50, 100, 150],\n",
        "    'max_features': ['sqrt', 'log2', None],\n",
        "    'max_depth': [3, 6, 9],\n",
        "    'max_leaf_nodes': [3, 6, 9],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "GMlu1Kq6C-Z_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "193e1ff2-d43a-47df-a449-7fb6777d13c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        16\n",
            "           1       1.00      0.94      0.97        18\n",
            "           2       0.92      1.00      0.96        11\n",
            "\n",
            "    accuracy                           0.98        45\n",
            "   macro avg       0.97      0.98      0.98        45\n",
            "weighted avg       0.98      0.98      0.98        45\n",
            "\n"
          ]
        }
      ],
      "source": [
        "RandomForest=RandomForestClassifier(n_estimators=100)\n",
        "RandomForest.fit(X_train,y_train)\n",
        "y_pred_Rf = RandomForest.predict(X_test)\n",
        "\n",
        "grid_RandomForest_class=GridSearchCV(estimator=RandomForest,param_grid=param_grid,scoring='accuracy',refit=True,return_train_score=True)\n",
        "\n",
        "grid_RandomForest_class.fit(X_train,y_train)\n",
        "y_predict= grid_RandomForest_class.predict(X_test)\n",
        "print(metrics.classification_report(y_test,y_predict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3N7TD7s_3Kp"
      },
      "source": [
        "*   1.4 Compare the best obtained results from 1.1 to 1.3 (use PrettyTable to dispaly the results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8em8QRGE1Z1",
        "outputId": "e935cfcd-f3c0-4cf9-cf08-153536ea87eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+--------------------+--------------------+--------------------+-------------------+--------------------+\n",
            "|               |        acc         |        pre         |       recall       |         f1        |     best score     |\n",
            "+---------------+--------------------+--------------------+--------------------+-------------------+--------------------+\n",
            "|      SVM      | 0.9777777777777777 | 0.9722222222222222 | 0.9814814814814815 | 0.975983436853002 | 0.980952380952381  |\n",
            "|      kNN      | 0.9777777777777777 | 0.9722222222222222 | 0.9814814814814815 | 0.975983436853002 | 0.961904761904762  |\n",
            "| Random Forest | 0.9777777777777777 | 0.9722222222222222 | 0.9814814814814815 | 0.975983436853002 | 0.9619047619047618 |\n",
            "+---------------+--------------------+--------------------+--------------------+-------------------+--------------------+\n"
          ]
        }
      ],
      "source": [
        "table = PrettyTable(['', 'acc', 'pre', 'recall', 'f1', 'best score'])\n",
        "\n",
        "table.add_row([\"SVM\", accuracy_score(y_test,y_predict), precision_score(y_test,y_predict,average='macro'), recall_score(y_test,y_predict,average='macro'), f1_score(y_test,y_predict,average='macro'), grid_svm_class.best_score_])\n",
        "table.add_row([\"kNN\", accuracy_score(y_test,y_predict), precision_score(y_test,y_predict,average='macro'), recall_score(y_test,y_predict,average='macro'), f1_score(y_test,y_predict,average='macro'), grid_KNN_class.best_score_])\n",
        "table.add_row([\"Random Forest\", accuracy_score(y_test,y_predict), precision_score(y_test,y_predict, average='macro'), recall_score(y_test,y_predict, average='macro'), f1_score(y_test,y_predict, average='macro'), grid_RandomForest_class.best_score_])\n",
        "print(table)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNv07ARGzOUm"
      },
      "source": [
        "#Task 2. \n",
        "For breast cancer dataset (https://tinyurl.com/3vme8hr3) which could be loaded from datasets in sklearn as follows:\n",
        "\n",
        "```\n",
        "#Import scikit-learn dataset library\n",
        "from sklearn import datasets\n",
        "\n",
        "#Load dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "```\n",
        "\n",
        "*   Apply **GridSearchCV** to different classification algorithms such as **SVM, kNN, LogisticRegression, RandomForest**.\n",
        "*   Compare the results obtained by the best hyperparameters among classification algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnoVB8J4vV36"
      },
      "source": [
        "*   2.1. Apply **GridSearchCV** to **SVM** \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "-ZTSvsJdvYqI"
      },
      "outputs": [],
      "source": [
        "# code\n",
        "from sklearn import datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "fjokgfScEKzt"
      },
      "outputs": [],
      "source": [
        "cancer = datasets.load_breast_cancer()\n",
        "X= cancer['data']\n",
        "y= cancer['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "r45cMTDZjErA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e6dcf40-21b1-4486-f110-5138e953f9d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.94      0.94        63\n",
            "           1       0.96      0.97      0.97       108\n",
            "\n",
            "    accuracy                           0.96       171\n",
            "   macro avg       0.96      0.95      0.96       171\n",
            "weighted avg       0.96      0.96      0.96       171\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n"
          ]
        }
      ],
      "source": [
        "param_grid = {'C': [0.1, 1, 10, 100, 1000],\n",
        "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
        "              'kernel': ['rbf','linear']}\n",
        "\n",
        "clf = svm.SVC(kernel='linear',random_state=0, max_iter=10000)\n",
        "grid_svm_class=GridSearchCV(estimator=clf,param_grid=param_grid,scoring='accuracy',refit=True,return_train_score=True)\n",
        "\n",
        "grid_svm_class.fit(X_train,y_train)\n",
        "y_predict= grid_svm_class.predict(X_test)\n",
        "print(metrics.classification_report(y_test,y_predict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3Xj2VrRD-Iq"
      },
      "source": [
        "#Task 2. \n",
        "For breast cancer dataset (https://tinyurl.com/3vme8hr3) which could be loaded from datasets in sklearn as follows:\n",
        "\n",
        "```\n",
        "#Import scikit-learn dataset library\n",
        "from sklearn import datasets\n",
        "\n",
        "#Load dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "```\n",
        "\n",
        "*   Apply **GridSearchCV** to different classification algorithms such as **SVM, kNN, LogisticRegression, RandomForest**.\n",
        "*   Compare the results obtained by the best hyperparameters among classification algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol1U_T_NvcqV"
      },
      "source": [
        "*   2.2. Apply **GridSearchCV** to **kNN** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "kt71yrAoBwYE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56ce2f10-97cc-4fa2-db7f-fb519487bb9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.94      0.95        63\n",
            "           1       0.96      0.98      0.97       108\n",
            "\n",
            "    accuracy                           0.96       171\n",
            "   macro avg       0.97      0.96      0.96       171\n",
            "weighted avg       0.96      0.96      0.96       171\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#code\n",
        "\n",
        "grid_params = { 'n_neighbors' : [5,7,9,11,13,15],\n",
        "               'weights' : ['uniform','distance'],\n",
        "               'metric' : ['minkowski','euclidean','manhattan']}\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred_knn = knn.predict(X_test)\n",
        "\n",
        "# accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
        "# precision_knn = precision_score(y_test, y_pred_knn, average='macro')\n",
        "# recall_knn = recall_score(y_test, y_pred_knn, average='macro')\n",
        "# f1_knn = f1_score(y_test, y_pred_knn, average='macro')\n",
        "\n",
        "grid_KNN_class=GridSearchCV(estimator=knn,param_grid=grid_params,scoring='accuracy',refit=True,return_train_score=True)\n",
        "\n",
        "grid_KNN_class.fit(X_train,y_train)\n",
        "y_predict= grid_KNN_class.predict(X_test)\n",
        "print(metrics.classification_report(y_test,y_predict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPkAvse-BxNa"
      },
      "source": [
        "*   2.3. Apply **GridSearchCV** to **LogisticRegression** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "nyYjpHFbB1Ci",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e444a4bf-5e9b-4aea-f893-ee62040c39b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.94      0.95        63\n",
            "           1       0.96      0.98      0.97       108\n",
            "\n",
            "    accuracy                           0.96       171\n",
            "   macro avg       0.97      0.96      0.96       171\n",
            "weighted avg       0.96      0.96      0.96       171\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#code\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "param_grid = {\n",
        "    'n_estimators': [25, 50, 100, 150],\n",
        "    'max_features': ['sqrt', 'log2', None],\n",
        "    'max_depth': [3, 6, 9],\n",
        "    'max_leaf_nodes': [3, 6, 9],\n",
        "}\n",
        "\n",
        "# Logistic Linear \n",
        "lr = LogisticRegression(random_state=42,solver='lbfgs', max_iter=10000)\n",
        "# lr.fit(X_train, y_train)\n",
        "# y_pred_lr = lr.predict(X_test)\n",
        "\n",
        "\n",
        "grid_lr_class=GridSearchCV(estimator=lr,param_grid=param_grid,scoring='accuracy',refit=True,return_train_score=True)\n",
        "\n",
        "# grid_lr_class.fit(X_train,y_train)\n",
        "# y_predict= grid_lr_class.predict(X_test)\n",
        "print(metrics.classification_report(y_test,y_predict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NjSLo5jB1xY"
      },
      "source": [
        "*   2.4. Apply **GridSearchCV** to **RandomForest** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "nktGtM0PB7XB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e898bef-e21b-4222-b1bf-69ca2b65d266"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.94      0.92        63\n",
            "           1       0.96      0.94      0.95       108\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.93      0.94      0.94       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#code\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [25, 50, 100, 150],\n",
        "    'max_features': ['sqrt', 'log2', None],\n",
        "    'max_depth': [3, 6, 9],\n",
        "    'max_leaf_nodes': [3, 6, 9],\n",
        "}\n",
        "\n",
        "RandomForest=RandomForestClassifier(n_estimators=100)\n",
        "RandomForest.fit(X_train,y_train)\n",
        "y_pred_Rf = RandomForest.predict(X_test)\n",
        "\n",
        "grid_RandomForest_class=GridSearchCV(estimator=RandomForest,param_grid=param_grid,scoring='accuracy',refit=True,return_train_score=True)\n",
        "\n",
        "grid_RandomForest_class.fit(X_train,y_train)\n",
        "y_predict= grid_RandomForest_class.predict(X_test)\n",
        "print(metrics.classification_report(y_test,y_predict))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZJ3BSHpB9Dx"
      },
      "source": [
        "*   2.5. Compare the best obtained results among classification algorithms (use PrettyTable to dispaly the results) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LS_IYfNCFEj"
      },
      "outputs": [],
      "source": [
        "#code\n",
        "\n",
        "table = PrettyTable(['', 'acc', 'pre', 'recall', 'f1', 'best score'])\n",
        "\n",
        "table.add_row([\"SVM\", accuracy_score(y_test,y_predict), precision_score(y_test,y_predict,average='macro'), recall_score(y_test,y_predict,average='macro'), f1_score(y_test,y_predict,average='macro'), grid_svm_class.best_score_])\n",
        "table.add_row([\"kNN\", accuracy_score(y_test,y_pred_knn), precision_score(y_test,y_pred_knn,average='macro'), recall_score(y_test,y_pred_knn,average='macro'), f1_score(y_test,y_pred_knn,average='macro'), grid_KNN_class.best_score_])\n",
        "# table.add_row([\"LogisticRegression\", accuracy_score(y_test,y_pred), precision_score(y_test,y_pred,average='macro'), recall_score(y_test,y_pred,average='macro'), f1_score(y_test,y_pred), grid_lr_class.best_score_])\n",
        "table.add_row([\"Random Forest\", accuracy_score(y_test,y_predict), precision_score(y_test,y_predict, average='macro'), recall_score(y_test,y_predict, average='macro'), f1_score(y_test,y_predict, average='macro'), grid_RandomForest_class.best_score_])\n",
        "\n",
        "print(table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b52OPWPD2afi"
      },
      "source": [
        "#Task 3. \n",
        "The dataset consists of **2000 user-created movie reviews** archived on the IMDb(Internet Movie Database). The reviews are equally partitioned into a positive set and a negative set (1000+1000). Each review consists of a plain text file (.txt) and a class label representing the overall user opinion. \n",
        "The class attribute has only two values: **pos** (positive) or **neg** (negative).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDcxOQRmDz_h"
      },
      "source": [
        "*   3.1 Importing additional libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjyW06skDwvL"
      },
      "outputs": [],
      "source": [
        "import nltk, random\n",
        "nltk.download('movie_reviews')#download movie reviews dataset\n",
        "from nltk.corpus import movie_reviews\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJpsTIiyv-1h"
      },
      "source": [
        "*   3.2. Movie reviews information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZE7A0Au1Pg0"
      },
      "outputs": [],
      "source": [
        "#code\n",
        "print(len(movie_reviews.fileids()))\n",
        "print(movie_reviews.categories())\n",
        "print(movie_reviews.words()[:100])\n",
        "print(movie_reviews.fileids()[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pHmMpqMHS23"
      },
      "source": [
        "*   3.3. Create dataset from movie reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45aY6woMHSH5"
      },
      "outputs": [],
      "source": [
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "random.seed(123)\n",
        "random.shuffle(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNke0Da5HqFa"
      },
      "outputs": [],
      "source": [
        "print('Number of Reviews/Documents: {}'.format(len(documents)))\n",
        "print('Corpus Size (words): {}'.format(np.sum([len(d) for (d,l) in documents])))\n",
        "print('Sample Text of Doc 1:')\n",
        "print('-'*30)\n",
        "print(' '.join(documents[0][0][:50])) # first 50 words of the first document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVFUEhnXHsGd"
      },
      "outputs": [],
      "source": [
        "sentiment_distr = Counter([label for (words, label) in documents])\n",
        "print(sentiment_distr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTXiEbMzHgVC"
      },
      "source": [
        "*   3.4. Train test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_-0gZZFHvJN"
      },
      "outputs": [],
      "source": [
        "train, test = train_test_split(documents, test_size = 0.33, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUGlm5TGHvpV"
      },
      "outputs": [],
      "source": [
        "## Sentiment Distrubtion for Train and Test\n",
        "print(Counter([label for (words, label) in train]))\n",
        "print(Counter([label for (words, label) in test]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1ppl_0RHx1P"
      },
      "outputs": [],
      "source": [
        "X_train = [' '.join(words) for (words, label) in train]\n",
        "X_test = [' '.join(words) for (words, label) in test]\n",
        "y_train = [label for (words, label) in train]\n",
        "y_test = [label for (words, label) in test]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xUaXrjxH6Ee"
      },
      "source": [
        "*   3.5. Text Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzwM0nsIH-8l"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "tfidf_vec = TfidfVectorizer(min_df = 10, token_pattern = r'[a-zA-Z]+')\n",
        "X_train_bow = tfidf_vec.fit_transform(X_train) # fit train\n",
        "X_test_bow = tfidf_vec.transform(X_test) # transform test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BP1vB3loIF28"
      },
      "source": [
        "*   3.6. Apply **SVM** with **GridSearchCV** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3FHQqh1Hlrd"
      },
      "outputs": [],
      "source": [
        "#code\n",
        "param_grid = {'C': [0.1, 1, 10, 100, 1000, 10000]}\n",
        "\n",
        "svm_class = svm.SVC(kernel='linear', random_state=0) \n",
        "grid_svm_Class = GridSearchCV(scoring = 'accuracy' ,estimator=svm_class, param_grid=param_grid, refit = True)\n",
        "\n",
        "grid_svm_Class.fit(X_train_bow, y_train)\n",
        "y_predict_svm = grid_svm_Class.predict(X_test_bow)\n",
        "print(metrics.classification_report(y_test,y_predict_svm))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1Fy8jYBIdxi"
      },
      "source": [
        "*   3.7. Apply **RandomForest** with **GridSearchCV** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Fyfw2R-gIhWl"
      },
      "outputs": [],
      "source": [
        "#code\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [25, 50, 100, 150],\n",
        "    'max_depth': [3, 6, 9],\n",
        "    'max_leaf_nodes': [3, 6, 9]\n",
        "}\n",
        "rf_class = RandomForestClassifier(n_estimators=100)\n",
        "grid_rf_Class = GridSearchCV(scoring = 'accuracy' ,estimator=rf_class, param_grid=param_grid, refit = True)\n",
        "\n",
        "\n",
        "grid_rf_Class.fit(X_train_bow, y_train)\n",
        "y_predict_rf = grid_rf_Class.predict(X_test_bow)\n",
        "print(metrics.classification_report(y_test,y_predict_rf))\n",
        "print(grid_rf_Class.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_btsVKjIIiLT"
      },
      "source": [
        "*   3.8. Apply **kNN** with **GridSearchCV** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "IZmFu1ZQImhn",
        "outputId": "09252ad2-f6d8-4204-c24a-66024a2666de"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-747d1161a0dc>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mgrid_kNN_Class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'accuracy'\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkNN_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrid_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mgrid_kNN_Class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_bow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_kNN_Class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_bow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    976\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    565\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    566\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#code\n",
        "\n",
        "grid_params = {'n_neighbors' : [5,7,9,11,13,15],\n",
        "               'weights' : ['uniform','distance'],\n",
        "               'metric' : ['minkowski','euclidean','manhattan']}\n",
        "kNN_class = KNeighborsClassifier()\n",
        "grid_kNN_Class = GridSearchCV(scoring = 'accuracy' ,estimator=kNN_class, n_jobs = -1, param_grid=grid_params, refit = True)\n",
        "\n",
        "grid_kNN_Class.fit(X_train_bow, y_train)\n",
        "y_predict = grid_kNN_Class.predict(X_test_bow)\n",
        "print(metrics.classification_report(y_test,y_predict))\n",
        "print(grid_kNN_Class.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ix_HeVGIvDu"
      },
      "source": [
        "*   3.9. Apply **LogisticRegression** with **GridSearchCV** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTd3alCMIr-i"
      },
      "outputs": [],
      "source": [
        "#code\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "param_grid = {\n",
        "    'n_estimators': [25, 50, 100, 150],\n",
        "    'max_depth': [3, 6, 9],\n",
        "    'max_leaf_nodes': [3, 6, 9]\n",
        "}\n",
        "lr_class = LogisticRegression(random_state=42,solver='lbfgs', max_iter=10000)\n",
        "grid_LogisticRegression_Class = GridSearchCV(scoring = 'accuracy' ,estimator=lr_class, param_grid=param_grid, refit = True)\n",
        "\n",
        "grid_LogisticRegression_Class.fit(X_train_bow, y_train)\n",
        "y_predict_rf = grid_LogisticRegression_Class.predict(X_test_bow)\n",
        "print(metrics.classification_report(y_test,y_predict_rf))\n",
        "print(grid_LogisticRegression_Class.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhYF2y6eI058"
      },
      "source": [
        "*   3.10. Compare the best obtained results among classification algorithms (use PrettyTable to dispaly the results) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgG7cb2aXKqT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok7RGkea_b7n"
      },
      "source": [
        "#Finally,\n",
        "Save a copy in your Github. Remember renaming the notebook."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}